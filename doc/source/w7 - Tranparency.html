<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="gothic.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic&display=swap">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="generator" content="pandoc">
    <meta name="description" content="">

    <title>w7 - Tranparency</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="https://maxcdn.bootstrapcdn.com/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="https://mushiyo.github.io/pandoc-toc-sidebar/css/dashboard.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">.sidebar ul{padding-left: 10px;}</style>
    <link rel="stylesheet" href="gothic.css" />
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">AI Ethics</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 1-2 Background <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w1 - AI History.html">w1 - AI History</a></li>
                <li><a href="w2 - Intro of Ethics.html">w2 - Intro of Ethics</a></li>
               
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 3-5 Ethics Type <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w3 - Utilitarianism.html">w3 - Utilitarianism</a></li>
                <li><a href="w4 - Deontology.html">w4 - Deontology</a></li>
                <li><a href="w5 - Virtue ethic.html">w5 - Virtue ethic</a></li>
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 6-7 & 9 分析方式<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w6 - Trust, justice, and accountability.html">w6 - Trust, justice, and accountability</a></li>
                <li><a href="w7 - Tranparency.html">w7 - Tranparency</a></li>
                <li><a href="w9 - Algorithmic Bias, Accessibility & Equity.html">w9 - Algorithmic Bias, Accessibility & Equity</a></li>
                
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 10-11 治理<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w10 - Data Governance.html">w10 - Data Governance</a></li>
                <li><a href="w11 - Explainable AI.html">w11 - Explainable AI</a></li>
              </ul>
            </li>        

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 8 Guest<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w8 - AI Ethic in Medicine.html">w8 - AI Ethic in Medicine</a></li>
              </ul>
          </ul>
        </div>
      </div>
    </nav>

    <div class="container-fluid">
      <div class="row">
        <div id="sidebar" class="col-sm-3 col-md-2 sidebar">
          <!--<ul class="nav nav-sidebar">
            <li class="active"><a href="#">Overview <span class="sr-only">(current)</span></a></li>
          </ul>-->
          <ul>
          <li><a href="#transparency"
          id="toc-transparency">Transparency</a>
          <ul>
          <li><a href="#what-is-ai-transparency"
          id="toc-what-is-ai-transparency">What is AI
          transparency</a></li>
          <li><a href="#three-dimensions-of-transparency"
          id="toc-three-dimensions-of-transparency">Three dimensions of
          transparency</a>
          <ul>
          <li><a href="#系统透明性system-transparency"
          id="toc-系统透明性system-transparency">系统透明性（System
          Transparency）</a></li>
          <li><a href="#程序透明性procedural-transparency"
          id="toc-程序透明性procedural-transparency">程序透明性（Procedural
          Transparency）</a></li>
          <li><a href="#结果透明性transparency-of-outcomes"
          id="toc-结果透明性transparency-of-outcomes">结果透明性（Transparency
          of Outcomes）</a></li>
          <li><a href="#example-hypothetical-thought-experiment."
          id="toc-example-hypothetical-thought-experiment.">Example:
          Hypothetical Thought Experiment.</a></li>
          </ul></li>
          <li><a href="#social-media-transparency"
          id="toc-social-media-transparency">Social Media
          Transparency</a>
          <ul>
          <li><a href="#内容审核与申诉content-moderation-and-censorship"
          id="toc-内容审核与申诉content-moderation-and-censorship">内容审核与申诉（Content
          Moderation and Censorship）</a></li>
          <li><a href="#隐私-data-privacy-and-usage"
          id="toc-隐私-data-privacy-and-usage">隐私 Data Privacy and
          Usage</a></li>
          <li><a href="#算法推荐-algorithmic-transparency"
          id="toc-算法推荐-algorithmic-transparency">算法推荐
          Algorithmic Transparency</a></li>
          <li><a href="#广告与赞助advertising-and-sponsored-content"
          id="toc-广告与赞助advertising-and-sponsored-content">广告与赞助（Advertising
          and Sponsored Content）</a></li>
          <li><a href="#impact-on-society-and-mental-health"
          id="toc-impact-on-society-and-mental-health">Impact on Society
          and Mental Health</a>
          <ul>
          <li><a
          href="#case-1-facebook-增加投票率实验2010年美国中期选举"
          id="toc-case-1-facebook-增加投票率实验2010年美国中期选举">Case
          1: Facebook 增加投票率实验（2010年美国中期选举）</a></li>
          <li><a
          href="#case-2-情绪传染实验emotional-contagion-study-2014"
          id="toc-case-2-情绪传染实验emotional-contagion-study-2014">Case
          2: 情绪传染实验（Emotional Contagion Study, 2014）</a></li>
          </ul></li>
          <li><a href="#ethical-recommendation"
          id="toc-ethical-recommendation">Ethical
          Recommendation</a></li>
          </ul></li>
          <li><a href="#criminal-justice-ai-systems"
          id="toc-criminal-justice-ai-systems">Criminal Justice AI
          Systems</a>
          <ul>
          <li><a href="#background"
          id="toc-background">Background</a></li>
          <li><a href="#issue-1-algorithm-bias"
          id="toc-issue-1-algorithm-bias">Issue 1 Algorithm
          bias</a></li>
          <li><a href="#issue-2-process-opacity"
          id="toc-issue-2-process-opacity">Issue 2 Process
          Opacity</a></li>
          </ul></li>
          <li><a href="#generative-ai" id="toc-generative-ai">Generative
          AI</a>
          <ul>
          <li><a href="#lack-of-transparency-in-training-data"
          id="toc-lack-of-transparency-in-training-data">Lack of
          Transparency in Training Data</a></li>
          <li><a href="#lack-of-transparency-in-decision-making"
          id="toc-lack-of-transparency-in-decision-making">Lack of
          transparency in decision making</a>
          <ul>
          <li><a href="#让ai自己解释它的行为self-justification"
          id="toc-让ai自己解释它的行为self-justification">让AI自己解释它的行为?（Self-Justification）</a></li>
          <li><a href="#下游含义-downstream-implications"
          id="toc-下游含义-downstream-implications">下游含义 DOWNSTREAM
          implications</a></li>
          </ul></li>
          <li><a href="#explainabilityinterpretability"
          id="toc-explainabilityinterpretability">Explainability/interpretability</a></li>
          </ul></li>
          <li><a href="#case-study-human-or-ai"
          id="toc-case-study-human-or-ai">Case Study: Human or AI?</a>
          <ul>
          <li><a href="#case-1-a-film-about-anthony-bourdain"
          id="toc-case-1-a-film-about-anthony-bourdain">Case 1: A Film
          About Anthony Bourdain</a></li>
          <li><a href="#case-2-human-like-chatbots"
          id="toc-case-2-human-like-chatbots">Case 2: Human-like
          Chatbots</a></li>
          <li><a href="#case-3-ai-generated-art"
          id="toc-case-3-ai-generated-art">Case 3: AI-Generated
          Art</a></li>
          <li><a href="#case-4-made-with-ai"
          id="toc-case-4-made-with-ai">Case 4: Made with AI</a></li>
          </ul></li>
          <li><a href="#improving-transparency"
          id="toc-improving-transparency">Improving
          Transparency</a></li>
          <li><a href="#mind-map" id="toc-mind-map">Mind map</a></li>
          </ul></li>
          </ul>
        </div>
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
        
<h1 id="transparency">Transparency</h1>
<h2 id="what-is-ai-transparency">What is AI transparency</h2>
<ol type="1">
<li><strong>在AI伦理中，透明性意味着开放、坦率，信息对用户是可理解的。</strong></li>
</ol>
<blockquote>
<p>AI or algorithmic transparency involves making the
<strong>functionalities, decision-making processes, and outcomes of AI
systems and algorithms</strong> <strong>clear and understandable to
users and stakeholders</strong>. It aims to ensure that AI operations
are open to scrutiny, <strong>promoting trust, ethical use, and</strong>
<strong>accountability</strong> in AI technologies.</p>
<p>AI或算法的透明性，是指让AI系统的功能、决策过程和输出结果对用户与利益相关者清晰可见、易于理解。这种透明性有助于提升信任、伦理使用与问责机制。</p>
</blockquote>
<blockquote>
<p>[!CAUTION]</p>
<p>在人机交互（HCI）中，“透明”指的是用户无感知地使用系统（例如一个操作自动完成而无需用户意识到）。</p>
<p>但在AI伦理中，我们是强调<strong>感知得到</strong>、<strong>看得懂</strong>、<strong>可被解释</strong>的透明性。</p>
</blockquote>
<ol start="2" type="1">
<li><strong>Transparency != tech issue</strong></li>
</ol>
<ul>
<li>透明性不是仅属于技术圈的议题，它在法律、隐私权、公民自由中也非常重要。</li>
<li>缺乏透明性的算法系统威胁到基本人权，例如表达自由、隐私安全。</li>
</ul>
<ol start="3" type="1">
<li><strong>Transparency != Explainability</strong>
<ul>
<li>Transparency
主要指系统<strong>整体运作过程</strong>的可追溯性。</li>
<li>而 Explainability
则是指<strong>算法本身内部逻辑的理解度</strong>（如神经网络是如何“想”的）。</li>
</ul></li>
<li><strong>系统视角：AI Transparency贯穿整个生命周期</strong></li>
</ol>
<p>从输入的数据、到模型的处理、再到输出后的使用与影响，每一步都应该透明、可审计。</p>
<blockquote>
<p>“It’s not just about the AI operational logic in the middle… It’s
about what’s coming in, what’s going out, and what happens
downstream.”</p>
</blockquote>
<ul>
<li>AI系统不仅仅包括中间的模型代码，还包括数据采集、输入处理、输出后的行为影响。</li>
<li>透明性要涵盖<strong>整个流程</strong>，而不仅是模型本身。</li>
</ul>
<hr />
<h2 id="three-dimensions-of-transparency">Three dimensions of
transparency</h2>
<p>本部分讲解 AI 透明性的三种核心维度：System Transparency, Procedural
Transparency</p>
<table style="width:100%;">
<colgroup>
<col style="width: 19%" />
<col style="width: 33%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr>
<th>维度</th>
<th>关注内容</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>System Transparency</td>
<td>系统怎么运行、数据来源、知识如何提取</td>
<td>模型架构、训练数据集信息</td>
</tr>
<tr>
<td>Procedural Transparency</td>
<td>系统在<mark>organisation</mark>中的使用方式</td>
<td>哪些人用AI、是否有审批机制、部署是否有偏差</td>
</tr>
<tr>
<td>Transparency of Outcomes</td>
<td>系统输出后发生了什么、影响了谁、是否可追踪</td>
<td>用户行为是否改变、这些改变是否in turn affect system development and
adaptations.</td>
</tr>
</tbody>
</table>
<h3 id="系统透明性system-transparency">系统透明性（System
Transparency）</h3>
<p>系统透明性主要关注<strong>AI系统本身的工作机制</strong>。包括：</p>
<ul>
<li>数据来源：系统用什么数据？</li>
<li>数据处理：如何提取知识？</li>
<li>算法逻辑：AI 是如何将数据转化为输出的？</li>
</ul>
<p>系统透明性是最基础的透明性，核心是让外部人可以知道这个AI系统“吃了什么数据”“怎么加工”和“做出什么”。它是对技术细节的整体概括，但不深入每一层算法内部运算（那属于Explainability）。</p>
<h3 id="程序透明性procedural-transparency">程序透明性（Procedural
Transparency）</h3>
<p>程序透明性指的是<strong>AI系统在具体场景中的运作流程</strong>。即使代码完全相同，不同组织中部署和使用方式可能完全不同，例如：</p>
<ul>
<li>谁在使用AI？</li>
<li>决策是自动执行还是人工辅助？</li>
<li>哪些流程能被复查或追责？</li>
</ul>
<p>他强调：</p>
<blockquote>
<p>“They might be used in different ways by two different organisations…
and that would then have to be considered in transparency
considerations.”</p>
</blockquote>
<hr />
<h3 id="结果透明性transparency-of-outcomes">结果透明性（Transparency of
Outcomes）</h3>
<p>结果透明性指的是<strong>让最终用户能够看到和理解AI带来的后果</strong>。</p>
<p>不只是“这次预测了什么”，还包括：</p>
<ul>
<li>用户行为有没有因为AI改变？</li>
<li>长期趋势是否发生变化？</li>
<li>这些变化如何反过来影响系统本身？</li>
</ul>
<p>结果透明性就是要让用户看得见“这个AI用了之后有什么效果”。是否改变了人的行为？是否对AI有影响？是否形成了闭环反馈？</p>
<h3 id="example-hypothetical-thought-experiment.">Example: Hypothetical
Thought Experiment.</h3>
<p>这个系统声称非常先进，考虑了大量因素，甚至有超级审计机构背书。但问题包括：</p>
<ul>
<li><p>使用了<strong>Nvidia 的私有算法</strong>（不可公开）</p></li>
<li><p><strong>评分结果不可质疑、不可申诉</strong> -&gt; No
Contestability</p></li>
<li><p>Black box -&gt; Lack of Explainability</p></li>
<li><p>无监督与审计 -&gt; Governance</p></li>
<li><p>是否知情 -&gt; Informed Consent</p></li>
</ul>
<p>假设你这门课的分数是由一个黑箱深度学习模型决定的，它参考了你看了多少伦理学视频、打字速度、你是否微笑对待助教等等，模型算法不能公开，你也不能质疑它的评分。</p>
<ul>
<li>U: agree</li>
<li>D: disagree - merely means</li>
</ul>
<hr />
<h2 id="social-media-transparency">Social Media Transparency</h2>
<p>本部分通过社交媒体平台的真实例子（如
Facebook、Twitter）说明缺乏透明性在实际生活中带来的问题，包括内容审核、数据使用、算法推荐、广告投放以及心理影响。</p>
<h3
id="内容审核与申诉content-moderation-and-censorship">内容审核与申诉（Content
Moderation and Censorship）</h3>
<blockquote>
<p>There’s often a <mark>lack of clear communication</mark> about
<mark>how and why certain content is moderated, removed, or
flagged</mark>. This includes the algorithms and criteria used for
content filtering and the processes available for
<mark>appeal</mark>.</p>
</blockquote>
<ul>
<li><p>社交平台经常删除、隐藏、屏蔽用户内容，但不告诉你原因。</p></li>
<li><p>审核机制和算法不透明，申诉流程也不清楚。</p></li>
</ul>
<h3 id="隐私-data-privacy-and-usage">隐私 Data Privacy and Usage</h3>
<blockquote>
<p>Social media platforms collect vast amounts of personal data, and
there are concerns about <mark>how transparently they disclose what data
is collected, how it is used, who it is shared with, and how users can
control their own data privacy settings</mark>.</p>
<p>社交媒体平台收集了大量的个人数据，并且担心他们在披露收集的数据，如何使用，与谁共享以及用户如何控制自己的数据隐私设置的透明性。</p>
<p>Terms of Service documentation is generally unwieldy</p>
<p>服务条款通常很笨拙</p>
</blockquote>
<p>用户的数据被收集了，但平台通常不会明确告诉你：</p>
<ul>
<li>收集了哪些内容？</li>
<li>用来干嘛？</li>
<li>分享给了谁？</li>
<li>怎么设置隐私？- 设置选项深藏在复杂UI中，用户不易找到或理解。</li>
</ul>
<h3 id="算法推荐-algorithmic-transparency">算法推荐 Algorithmic
Transparency</h3>
<blockquote>
<p>The algorithms that <mark>determine what content is shown</mark> to
which users can influence public opinion and even election outcomes.
<mark>However</mark>, the workings of these <mark>algorithms are often
proprietary and not disclosed to the public, raising questions about
bias, manipulation, and accountability.</mark></p>
</blockquote>
<ul>
<li>推荐内容的算法可以操纵舆论，例如通过放大某种政治观点影响选举。</li>
<li>但这些算法是平台的商业秘密，公众无法检查其公正性。</li>
</ul>
<h3
id="广告与赞助advertising-and-sponsored-content">广告与赞助（Advertising
and Sponsored Content）</h3>
<blockquote>
<p>It can sometimes be <mark>difficult for users to distinguish between
organic content and sponsored content</mark>, including political
advertising. There’s a call for clearer labelling and transparency
regarding <mark>who is paying for ads and how targeting decisions are
made.</mark></p>
<p>Why is one seeing the ad that they are seeing?</p>
</blockquote>
<ul>
<li>平台可能会显示带有“你感兴趣”的广告，但你无法知道广告主是如何定位你的。</li>
<li>有时广告和普通内容混在一起，缺乏标识，容易误导用户。</li>
</ul>
<h3 id="impact-on-society-and-mental-health">Impact on Society and
Mental Health</h3>
<blockquote>
<p>There are concerns about the impact of social media on societal
processes (e.g. voting) and individual mental health. Transparency
regarding the research conducted by platforms and the measures taken to
mitigate negative effects is an area of public interest.</p>
<p>人们担心社交媒体对社会过程（例如投票）和个人心理健康的影响。关于平台进行的研究以及减轻负面影响的措施的透明度是公众关注的领域。</p>
</blockquote>
<p>社交平台上发生的实验可能在不知情下影响：</p>
<ul>
<li>选民投票行为（Election turnout）</li>
<li>情绪变化（Emotional contagion）</li>
</ul>
<h4 id="case-1-facebook-增加投票率实验2010年美国中期选举">Case 1:
Facebook 增加投票率实验（2010年美国中期选举）</h4>
<p>Facebook
在未经用户同意的情况下做了大规模社会实验，该实验用不同方式鼓励用户投票，直接干预了选举，影响范围达数十万人。</p>
<p>没有获得用户同意，也没有事后告知。</p>
<h4 id="case-2-情绪传染实验emotional-contagion-study-2014">Case 2:
情绪传染实验（Emotional Contagion Study, 2014）</h4>
<p>操纵689,003位用户的新闻提要，以改变他们看到的正面和负面帖子的平衡，目的是观察平台上其情感表达的变化。</p>
<p>该研究发现slight changes in users’ emotion，</p>
<ul>
<li>缺乏参与者的consent</li>
<li>这种操纵的potential psychological impacts</li>
<li>未经伦理审查（no IRB approval）</li>
</ul>
<h3 id="ethical-recommendation">Ethical Recommendation</h3>
<p>为了加强ethical approval processes for social media experiments, we
recommend:</p>
<ol type="1">
<li><p>We may need to consider <strong>not just the impact on the
individual</strong> under study <strong>but</strong> the broader
<strong>impact</strong> any experiment <strong>might have on
society</strong>
实验不能只考虑个体影响，还要考虑整个社会的连锁效应；</p>
<ul>
<li><p>For voting study，这可能是选举风险。</p></li>
<li><p>For fake news studt，它可能会降低社会中真正新闻的信任。</p></li>
<li><p>For emotional control study,
可能是所研究人群的情感福祉。</p></li>
</ul></li>
<li><p>即使是历史数据，二次使用也需要伦理审批 ethics approval may be
needed</p></li>
<li><p>即使不能事前告知，也应在实验后告知用户 inform after the
study</p></li>
</ol>
<hr />
<h2 id="criminal-justice-ai-systems">Criminal Justice AI Systems</h2>
<p>透明性问题在刑事司法AI系统中的表现。本节分析AI在刑事司法中的应用——如风险评估、预测性警务、面部识别——如何因缺乏透明性而引发严重伦理问题，并以
OASys、COMPAS 和 PredPol 为具体案例进行说明。</p>
<h3 id="background">Background</h3>
<p>Risk Assessment Tools：预测嫌犯是否会再次犯罪</p>
<p>Predictive Policing：预测哪个地区更可能发生犯罪</p>
<p>Facial Recognition：用于识别监控视频中的嫌疑人、受害者或证人。</p>
<h3 id="issue-1-algorithm-bias">Issue 1 Algorithm bias</h3>
<p>What if history data have bias?</p>
<p>如果历史数据本身有偏见（比如某些种族被过度执法），AI系统就会“继承”这些偏见，进而不断强化它，最终造成<strong>自我实现预言（self-fulfilling
prophecy）</strong>。</p>
<h3 id="issue-2-process-opacity">Issue 2 Process Opacity</h3>
<p>即使系统本身是“可解释的”，如果部署方式、使用目的、责任归属不明确，仍然是不透明的；</p>
<blockquote>
<p>“We tend to forget that transparency also includes how the system is
deployed, how it is used, and how decisions are made from it.”</p>
</blockquote>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 64%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>问题类别</th>
<th>中文说明</th>
<th>案例</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据偏见</td>
<td>AI沿用有偏的历史数据，放大对少数群体的不公</td>
<td>COMPAS、PredPol</td>
</tr>
<tr>
<td>黑箱决策</td>
<td>算法打分机制不公开，影响司法判决</td>
<td>OASys</td>
</tr>
<tr>
<td>反馈循环</td>
<td>AI预测 → 增加执法 → 增加犯罪记录 → AI继续预测</td>
<td>PredPol</td>
</tr>
<tr>
<td>过程不透明</td>
<td>谁用AI、怎么用、谁负责不明确</td>
<td>所有三个系统</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="generative-ai">Generative AI</h2>
<p>本节聚焦<mark>生成式AI</mark>（如 ChatGPT、DALL·E、Gemini ）</p>
<h3 id="lack-of-transparency-in-training-data">Lack of Transparency in
Training Data</h3>
<blockquote>
<p>Lack of transparency in <strong>the data sets used to train</strong>
generative AI systems. These systems are often trained on large
<strong>data sets</strong>, which <strong>can contain biases</strong>
and other issues that can affect the output of the system.
<strong>Without transparency</strong> in the data sets, it can be
difficult to identify and address these issues.</p>
</blockquote>
<ol type="1">
<li><strong>Data Provenance data来源和合理性</strong>
<ul>
<li>Author: I didn’t give premission!</li>
</ul></li>
<li><strong>Bias and Fairness</strong>
<ul>
<li>他会模仿或放大现实中的不公</li>
<li>很难评估和纠正偏见</li>
</ul></li>
</ol>
<blockquote>
<p>“Without insight into the datasets, it’s <strong>difficult to assess
and correct for biases</strong>… AI models may perpetuate or
<strong>amplify societal biases.</strong></p>
</blockquote>
<blockquote>
<p>[!TIP]</p>
<p>如果训练数据中男性领导者更多，那AI可能更倾向于将“领导”与男性联系；</p>
<p>如果社交媒体帖子本身就带有性别、种族偏见，那么AI生成的内容也可能被污染。</p>
</blockquote>
<ol start="3" type="1">
<li><strong>Ethical Use and Misuse</strong></li>
</ol>
<blockquote>
<p>Without transparency, there’s a risk that datasets may include
unethical, illegal, or harmful content. This lack of oversight can lead
to generative AI systems that inadvertently generate inappropriate or
harmful content.</p>
</blockquote>
<blockquote>
<p>[!TIP]</p>
<p>CHATGPT雇佣肯尼亚工人用极低工资清洗“有毒训练数据”，但造成了创伤与过劳；</p>
</blockquote>
<h3 id="lack-of-transparency-in-decision-making">Lack of transparency in
decision making</h3>
<blockquote>
<p>Lack of transparency in the decision-making processes used by
generative AI systems. These systems can make decisions based on complex
calculations and analysis, but it <strong>can be difficult to understand
how these decisions are being made</strong>. This can be particularly
concerning in cases where the system is making decisions that can have a
significant impact on people’s lives, such as in medical diagnosis or
hiring decisions.</p>
</blockquote>
<h4
id="让ai自己解释它的行为self-justification">让AI自己解释它的行为?（Self-Justification）</h4>
<blockquote>
<p>“To ask the system itself to give you an explanation might be OK, but
also questionable… <strong>perhaps an external system should do
that.</strong>”</p>
</blockquote>
<h4 id="下游含义-downstream-implications">下游含义 DOWNSTREAM
implications</h4>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr>
<th>implications</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trust</td>
<td>自己都不理解<br />If users and stakeholders cannot understand how
decisions are made, it becomes difficult to trust the system.</td>
</tr>
<tr>
<td>Accountability</td>
<td>没有透明，找谁问责都不知道<br />Without transparency, it’s
challenging to hold the system or its creators accountable for errors or
biased outcomes.”</td>
</tr>
<tr>
<td>Regulatory Compliance</td>
<td>很多行业（如医疗、金融）都有强制性法规要求透明决策流程。如果AI系统黑箱化，就有违反法律风险。<br />“Many
industries are subject to <mark>regulatory standards</mark> that require
transparency… AI systems that lack transparency may fail to comply</td>
</tr>
<tr>
<td>Safety and Reliability</td>
<td>Without transparency, it’s <mark>difficult to predict and mitigate
failures</mark> or dangerous behaviours.</td>
</tr>
<tr>
<td>Improvement and Innovation</td>
<td>Transparency allows developers to understand and improve the
decision-making processes of AI systems. Without insight into how
decisions are made, it becomes <mark>harder to refine and enhance the
system.</mark></td>
</tr>
<tr>
<td>Downstream</td>
<td>“If app A uses GPT output… and app B uses A’s results… who is
accountable?”</td>
</tr>
</tbody>
</table>
<h3
id="explainabilityinterpretability">Explainability/interpretability</h3>
<p>Focus of upcoming lecture</p>
<hr />
<h2 id="case-study-human-or-ai">Case Study: Human or AI?</h2>
<p>当AI系统“伪装成人类”或模仿人类行为时，是否应该明确披露其身份？</p>
<h3 id="case-1-a-film-about-anthony-bourdain">Case 1: A Film About
Anthony Bourdain</h3>
<p>Background</p>
<ul>
<li>Anthony Bourdain 是著名美食旅行纪录片主持人，于 2018 年去世；</li>
<li>纪录片中使用了他的书面文字，但用 AI 模拟出他“说话的声音”；</li>
<li>实际上他从未说过这些话，观众却以为他“生前说过”。</li>
</ul>
<p>Analysis</p>
<ol type="1">
<li>Play existing recordings; -&gt; acceptable</li>
<li>Use AI to voice things they wrote; -&gt; Controversial</li>
<li>Use AI to say things they never wrote.” -&gt; not acceptable</li>
</ol>
<h3 id="case-2-human-like-chatbots">Case 2: Human-like Chatbots</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI客服是否应该主动告知“我不是人类”？</td>
<td>Should AI systems disclose or make it manifestly clear that they are
not human?</td>
</tr>
<tr>
<td>AI头像是否应该避免进入“恐怖谷”（Uncanny Valley）？</td>
<td>Should service bots not move beyond the uncanny valley?</td>
</tr>
<tr>
<td>人类是否也应该明确声明“我是人类”？（inverse uncanny valley）</td>
<td>Should humans disclose that they are humans? (inverse uncanny
valley)</td>
</tr>
</tbody>
</table>
<h3 id="case-3-ai-generated-art">Case 3: AI-Generated Art</h3>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI生成的画是否算“真正的艺术”？</td>
<td>“Can AI-generated content be art in any standard sense?”<br/></td>
</tr>
<tr>
<td>作品如果是AI生成，是否应<strong>强制标注</strong>“由AI创作”？</td>
<td>“Should AI-generated art always be labelled as such?”</td>
</tr>
</tbody>
</table>
<p>如果观众不知道一幅作品是AI生成的，可能会被误导，以为是人类艺术家的创意结晶。这不仅伤害了真正艺术家的权益，也让“艺术真实性”概念模糊不清。</p>
<h3 id="case-4-made-with-ai">Case 4: Made with AI</h3>
<ul>
<li>平台正在开始要求将“AI生成或修改过的内容”进行标注；</li>
<li>例如：如果一个视频中的人脸是通过AI换脸技术合成的，必须在视频旁边注明。</li>
</ul>
<h2 id="improving-transparency">Improving Transparency</h2>
<table>
<tbody>
<tr>
<td>AI Explainability</td>
</tr>
<tr>
<td>Open Sourcing</td>
</tr>
<tr>
<td>Audit Trails 审计</td>
</tr>
<tr>
<td>Impact Assessments 评估影响</td>
</tr>
<tr>
<td>Stakeholder Engagement 纳入多方stakeholders的观点</td>
</tr>
<tr>
<td>Certification and Labelling 建立label</td>
</tr>
<tr>
<td>Education and Awareness 提升教育</td>
</tr>
</tbody>
</table>
<h2 id="mind-map">Mind map</h2>
<p><strong>Transparency: what’s it all about?</strong></p>
<ul>
<li>让AI系统的功能、决策过程和输出结果对用户与利益相关者清晰可见、易于理解。</li>
<li>三个dimension：
<ul>
<li>System -&gt; AI 算法本身是否透明</li>
<li>Procedural -&gt; AI系统在具体场景中的运作流程。
<ul>
<li>即使代码完全相同，不同组织中部署和使用方式可能完全不同</li>
<li>Diff organisation, deploy</li>
</ul></li>
<li>Outcomes -&gt; 这个AI用了之后有什么效果”。
<ul>
<li>是否改变了人的行为？是否对AI有影响？是否形成了闭环反馈？</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Automated decision-making (ADM) systems: transparency at
every stage?</strong></p>
<ul>
<li>他是贯穿整个生命周期的。</li>
</ul>
<p><strong>Current issues in transparency and stakeholders’
concerns:</strong></p>
<ol type="1">
<li>social media (advertising) systems
<ul>
<li>Content Moderation and Censorship - how does the algorithm
work?</li>
<li>Data Privacy and Usage</li>
<li>算法推荐 Algorithmic Transparency -&gt; 操纵用户</li>
<li>广告</li>
<li>实验:
<ul>
<li>实验不能只考虑个体影响，还要考虑整个社会的连锁效应；</li>
<li>即使是历史数据，二次使用也需要伦理审批 ethics approval may be
needed</li>
<li>即使不能事前告知，也应在实验后告知用户 inform after the study</li>
</ul></li>
</ul></li>
<li>criminal justice AI systems</li>
<li>generative AIs</li>
</ol>
<p>Human/AI transparency distinctions.</p>
<ul>
<li>self-fulfilling prophecy</li>
</ul>
<p>Overview of regulatory frameworks.</p>
        </div>
      </div>
    </div>
    
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="https://maxcdn.bootstrapcdn.com/js/ie10-viewport-bug-workaround.js"></script>
    <script>
        //document.getElementById('sidebar').getElementsByTagName('ul')[0].className += "nav nav-sidebar";
        
        /* ajust the height when click the toc
           the code is from https://github.com/twbs/bootstrap/issues/1768
        */
        var shiftWindow = function() { scrollBy(0, -50) };
        window.addEventListener("hashchange", shiftWindow);
        function load() { if (window.location.hash) shiftWindow(); }
        
        /*add Bootstrap styles to tables*/
        var tables = document.getElementsByTagName("table");
        for(var i = 0; i < tables.length; ++i){
            tables[i].className += "table table-bordered table-hover";
        }

        /* Add quote styles */
  
        document.addEventListener("DOMContentLoaded", function () {
          document.querySelectorAll("blockquote").forEach(function (block) {
            const first = block.querySelector("p");
            if (!first) return;
            const text = first.textContent.trim();

            if (text.startsWith("[!NOTE]")) {
              block.classList.add("note");
              first.innerHTML = "<strong>NOTE</strong>" + first.innerHTML.replace("[!NOTE]", "");
            } else if (text.startsWith("[!TIP]")) {
              block.classList.add("tip");
              first.innerHTML = "<strong>TIP</strong>" + first.innerHTML.replace("[!TIP]", "");
            } else if (text.startsWith("[!WARNING]")) {
              block.classList.add("warning");
              first.innerHTML = "<strong>WARNING</strong>" + first.innerHTML.replace("[!WARNING]", "");
            }
          });
        });

        document.addEventListener('DOMContentLoaded', function () {
          var links = document.querySelectorAll('a[href]');
          for (var i = 0; i < links.length; i++) {
            var href = links[i].getAttribute('href');
            if (!href) continue;

            if (href.indexOf('.md#') !== -1) {
              var parts = href.split('.md#');
              var newHref = parts[0] + '.html#' + parts[1];
              links[i].setAttribute('href', newHref);
            } else if (href.length > 3 && href.slice(-3) === '.md') {
              var newHref = href.slice(0, -3) + '.html';
              links[i].setAttribute('href', newHref);
            }
          }
        });




    </script>
  </body>
</html>
