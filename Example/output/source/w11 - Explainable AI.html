<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="gothic.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic&display=swap">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="generator" content="pandoc">
    <meta name="description" content="">

    <title>w11 - Explainable AI</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="https://maxcdn.bootstrapcdn.com/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="https://mushiyo.github.io/pandoc-toc-sidebar/css/dashboard.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">.sidebar ul{padding-left: 10px;}</style>
    <link rel="stylesheet" href="gothic.css" />
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">AI Ethics</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 1-2 Background <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w1 - AI History.html">w1 - AI History</a></li>
                <li><a href="w2 - Intro of Ethics.html">w2 - Intro of Ethics</a></li>
               
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 3-5 Ethics Type <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w3 - Utilitarianism.html">w3 - Utilitarianism</a></li>
                <li><a href="w4 - Deontology.html">w4 - Deontology</a></li>
                <li><a href="w5 - Virtue ethic.html">w5 - Virtue ethic</a></li>
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 6-7 & 9 分析方式<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w6 - Trust, justice, and accountability.html">w6 - Trust, justice, and accountability</a></li>
                <li><a href="w7 - Tranparency.html">w7 - Tranparency</a></li>
                <li><a href="w9 - Algorithmic Bias, Accessibility & Equity.html">w9 - Algorithmic Bias, Accessibility & Equity</a></li>
                
              </ul>
            </li>

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 10-11 治理<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w10 - Data Governance.html">w10 - Data Governance</a></li>
                <li><a href="w11 - Explainable AI.html">w11 - Explainable AI</a></li>
              </ul>
            </li>        

            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Weeks 8 Guest<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="w8 - AI Ethic in Medicine.html">w8 - AI Ethic in Medicine</a></li>
              </ul>
          </ul>
        </div>
      </div>
    </nav>

    <div class="container-fluid">
      <div class="row">
        <div id="sidebar" class="col-sm-3 col-md-2 sidebar">
          <!--<ul class="nav nav-sidebar">
            <li class="active"><a href="#">Overview <span class="sr-only">(current)</span></a></li>
          </ul>-->
          <ul>
          <li><a href="#explainable-ai"
          id="toc-explainable-ai">Explainable AI</a>
          <ul>
          <li><a href="#why-ask-why" id="toc-why-ask-why">Why ask
          why?</a>
          <ul>
          <li><a href="#what-is-explainable-ai"
          id="toc-what-is-explainable-ai">What is Explainable
          AI？</a></li>
          <li><a href="#who-care-xai" id="toc-who-care-xai">Who care
          XAI?</a></li>
          </ul></li>
          <li><a href="#the-challenges-of-xai"
          id="toc-the-challenges-of-xai">The challenges of XAI</a>
          <ul>
          <li><a href="#opacity-and-black-boxes"
          id="toc-opacity-and-black-boxes">Opacity and ‘Black
          Boxes</a></li>
          <li><a href="#causality-因果关系缺失"
          id="toc-causality-因果关系缺失">Causality
          因果关系缺失</a></li>
          <li><a href="#the-human-problem"
          id="toc-the-human-problem">The Human Problem</a></li>
          </ul></li>
          <li><a href="#properties-of-xai-approaches"
          id="toc-properties-of-xai-approaches">Properties of XAI
          approaches</a>
          <ul>
          <li><a href="#local-vs-global-explanation"
          id="toc-local-vs-global-explanation">Local vs Global
          Explanation</a></li>
          <li><a
          href="#interpretability-vs-post-hoc-explanation可解释性-vs-后验解释"
          id="toc-interpretability-vs-post-hoc-explanation可解释性-vs-后验解释">Interpretability
          vs Post-hoc Explanation（可解释性 vs 后验解释）</a></li>
          <li><a href="#example-decision-tree-vs-neural-network"
          id="toc-example-decision-tree-vs-neural-network">Example:
          Decision Tree vs Neural Network</a></li>
          <li><a href="#model-agnostic-vs-model-specific"
          id="toc-model-agnostic-vs-model-specific">Model-Agnostic vs
          Model-Specific</a></li>
          </ul></li>
          <li><a href="#explainable-ai-methods"
          id="toc-explainable-ai-methods">Explainable AI Methods</a>
          <ul>
          <li><a href="#rule-based-explanation"
          id="toc-rule-based-explanation">1. Rule-based Explanation</a>
          <ul>
          <li><a href="#limitations"
          id="toc-limitations">Limitations</a></li>
          </ul></li>
          <li><a href="#attribution-based-explanations"
          id="toc-attribution-based-explanations">2. Attribution-based
          Explanations</a>
          <ul>
          <li><a href="#limitations-1"
          id="toc-limitations-1">Limitations</a></li>
          </ul></li>
          <li><a href="#example-based-explanations"
          id="toc-example-based-explanations">3. Example-based
          Explanations</a>
          <ul>
          <li><a href="#method-1prototypes-and-criticisms"
          id="toc-method-1prototypes-and-criticisms">Method
          1:<strong>Prototypes and Criticisms</strong></a></li>
          <li><a href="#method-2-counterfactual-explanations"
          id="toc-method-2-counterfactual-explanations">Method 2:
          <strong>Counterfactual Explanations</strong></a></li>
          <li><a href="#limitations-2"
          id="toc-limitations-2">Limitations</a></li>
          <li><a href="#method-3-contrastive-explanations"
          id="toc-method-3-contrastive-explanations">Method 3:
          <strong>Contrastive Explanations</strong></a></li>
          </ul></li>
          </ul></li>
          <li><a href="#ethical-philosophical-considerations"
          id="toc-ethical-philosophical-considerations">Ethical &amp;
          Philosophical Considerations</a>
          <ul>
          <li><a href="#principles-for-ai-in-society"
          id="toc-principles-for-ai-in-society">Principles for AI in
          Society</a></li>
          </ul></li>
          <li><a href="#mind-map" id="toc-mind-map">Mind Map</a></li>
          </ul></li>
          </ul>
        </div>
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
        
<h1 id="explainable-ai">Explainable AI</h1>
<h2 id="why-ask-why">Why ask why?</h2>
<p>你是一名成绩优异的毕业生，申请了一个理想工作岗位，30
秒后就收到了拒绝邮件——理由完全不清楚。你想知道为什么，但公司告诉你：“我们使用高级机器学习算法做决定，无法提供解释。-&gt;
we need explain!</p>
<h3 id="what-is-explainable-ai">What is Explainable AI？</h3>
<p>Explainable AI 是一类 AI
技术和方法，它们可以解释机器学习系统是如何得出决策的。</p>
<blockquote>
<p>Explainable Artificial Intelligence (XAI) refers to methods and
techniques in the field of AI that provide more understanding of AI
(machine learning) systems and how they reach their decisions.</p>
</blockquote>
<ol type="1">
<li><p><strong>Transparency（透明性）</strong>：</p>
<blockquote>
<p>“XAI helps in making the decision-making processes of AI systems
transparent.” 它使得模型的决策过程对用户和开发者可见。</p>
</blockquote></li>
<li><p><strong>Trust &amp; Confidence（建立信任）</strong>：</p>
<blockquote>
<p>“Can increase user trust and confidence in AI applications,
especially in critical areas.”
在医疗、金融、司法等高风险领域尤其重要。</p>
</blockquote></li>
<li><p><strong>Regulatory Compliance（法律合规）</strong>：</p>
<blockquote>
<p>“Help ensure compliance with… such as GDPR’s right to explanation.”
比如 GDPR（通用数据保护条例）规定用户有“解释权”。</p>
</blockquote></li>
</ol>
<h3 id="who-care-xai">Who care XAI?</h3>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 48%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr>
<th>Stakeholder</th>
<th>原文举例与说明</th>
<th>中文说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Scientists</td>
<td>“Understand, debug, improve model performance.”</td>
<td>想了解和改进模型性能的<strong>技术专家</strong></td>
</tr>
<tr>
<td>Business Owners</td>
<td>“Evaluate suitability, accept use.”</td>
<td>关心<strong>商业</strong>可行性与<strong>公司</strong>责任的决策者</td>
</tr>
<tr>
<td>Risk Modellers</td>
<td>“Challenge model, ensure robustness.”</td>
<td>做模型<strong>审核</strong>与验证的风险分析人员</td>
</tr>
<tr>
<td>Regulators</td>
<td>“Check impact, verify reliability.”</td>
<td><strong>监管</strong>人员，确保合法性与公平性</td>
</tr>
<tr>
<td>Consumers</td>
<td>“What’s the impact on me? What actions can I take?”</td>
<td><strong>最终用户</strong>，想知道影响与应对方法</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="the-challenges-of-xai">The challenges of XAI</h2>
<h3 id="opacity-and-black-boxes">Opacity and ‘Black Boxes</h3>
<p>“Opacity”
指系统不透明、不可看穿，正好与“transparency（透明性）”相反。黑箱模型就是典型的
opaque system。</p>
<p>Black box 的问题在于，While it works, it doesn’t explain why it
works, which is unacceptable in high-risk scenarios (e.g., medical,
judicial).</p>
<h3 id="causality-因果关系缺失">Causality 因果关系缺失</h3>
<blockquote>
<p>“Machine learning excels at <mark>finding correlations</mark>… but
not grounded in <mark>causal reasoning</mark>.” →
机器学习善于找到相关性，但通常无法推断因果关系。</p>
</blockquote>
<ul>
<li><p>Lifestyle → Smoking &amp; Alcohol</p>
<p>Smoking → Lung Cancer</p>
<p>Alcohol 与 Lung Cancer
的关系只是相关（correlation），不是因果（causation）</p></li>
</ul>
<h3 id="the-human-problem">The Human Problem</h3>
<blockquote>
<p>“Much if not most of XAI is driven by the desire of AI experts to
better understand their models…” <mark>But we also need explanations for
non-experts.</mark> XAI
的研究往往<strong>以技术人员为中心</strong>，关注调试、优化模型，但忽略了非技术用户的理解需求。</p>
</blockquote>
<hr />
<h2 id="properties-of-xai-approaches">Properties of XAI approaches</h2>
<h3 id="local-vs-global-explanation">Local vs Global Explanation</h3>
<p>Local is 对<strong>某一个输入实例</strong>的解释, Global is
对<strong>整个模型行为</strong>的解释</p>
<ul>
<li><p><strong>Local explanation</strong>: refers to insights that
explain the <strong>decision-making process</strong> of an AI</p>
<p>model <strong>for a specific instance or input</strong> (e.g. why did
<em>my</em> application get rejected?)</p></li>
<li><p><strong>Global explanation</strong>: refers to insights that
explain the <strong>overall behaviour and decision- making
process</strong> of an AI model <strong>across all possible inputs and
scenarios</strong> (e.g. how does this model make decisions across some
consumer population?)</p></li>
</ul>
<h3
id="interpretability-vs-post-hoc-explanation可解释性-vs-后验解释">Interpretability
vs Post-hoc Explanation（可解释性 vs 后验解释）</h3>
<p>Interpretability: 模型<strong>本身</strong>易于理解 inherently
understandable.</p>
<p>Explanation: explanation of <mark>output</mark> and decision making
<mark>（Black Box）</mark></p>
<p>Post-hoc:
模型已经<strong>训练好并给出预测结果之后</strong>，我们再试图解释这个结果是如何产生的。</p>
<p>从某种意义上说，Interpretability意味着Explanation，但Explanation并不一定意味着Interpretability</p>
<ul>
<li>一个黑箱模型可以用 post-hoc
方法解释输出，但这并不意味着你真的理解了模型本身。</li>
</ul>
<h3 id="example-decision-tree-vs-neural-network">Example: Decision Tree
vs Neural Network</h3>
<p><strong>Interpretable：Decision Tree</strong></p>
<ul>
<li>输入一些特征，如“是否有刺”、“翅膀数量”、“眼睛数”</li>
<li>模型路径清晰 → 输出结果为“Bee”或“Fly”等</li>
</ul>
<p><strong>Post-hoc 示例：神经网络识别狼和哈士奇</strong></p>
<ul>
<li>模型训练时意外学会了“背景雪地 = 狼”</li>
<li>一张哈士奇照片因背景有雪被错判为狼</li>
</ul>
<h3 id="model-agnostic-vs-model-specific">Model-Agnostic vs
Model-Specific</h3>
<ul>
<li><p><strong>Model-specific</strong>: refers to methods that are
designed to provide insights or explanations</p>
<p>tailored to the internal mechanisms and architecture of a specific
type of AI model.</p>
<ul>
<li>inner workings of model used for explanation</li>
<li>依赖于具体模型结构（如神经网络、决策树）</li>
</ul></li>
<li><p><strong>Model-agnostic</strong>: refers to methods that can
provide insights or explanations for any AI model’s decisions,
regardless of the model’s internal architecture or workings.</p>
<ul>
<li>不依赖模型内部结构，只用输入和输出</li>
</ul></li>
</ul>
<p>‒ uses only inputs and outputs for explanation ‒ applicable to any
model with that interface</p>
<hr />
<h2 id="explainable-ai-methods">Explainable AI Methods</h2>
<h3 id="rule-based-explanation">1. Rule-based Explanation</h3>
<p>使用明确的 <mark style="background-color: #90ee90;"> if-then </mark>
规则 来解释模型是如何从输入推理出输出的。</p>
<blockquote>
<p>“Use of explicit rules to describe how a model makes decisions.”</p>
</blockquote>
<h4 id="limitations">Limitations</h4>
<p>不适用于复杂任务</p>
<h3 id="attribution-based-explanations">2. Attribution-based
Explanations</h3>
<blockquote>
<p>“Determining the contribution of individual input features to the
output.”
衡量输入中<strong>哪些部分</strong>最重要，最负责模型的预测结果。</p>
</blockquote>
<blockquote>
<p>[!TIP]</p>
<p>我们训练了一个
<strong>图像分类模型</strong>，用来识别图像中是什么动物，比如狗、猫、马。</p>
<p>你输入了一张狗的照片，模型输出：“狗 🐶”。</p>
<p>你想知道：“它为什么认为这是一只狗？”</p>
<p>Saliency map 把模型最关注的区域用红色/高亮标出来。</p>
</blockquote>
<blockquote>
<p>[!TIP]</p>
<blockquote>
<p>“The food was absolutely terrible and the waiter was rude.”</p>
</blockquote>
<p>你用一个情感分析模型，它输出：<strong>Negative</strong>（负面情绪）</p>
<p>Attribution 方法（如 SHAP）会告诉你：</p>
<ul>
<li>“terrible” 贡献了 70% 的负面</li>
<li>“rude” 贡献了 20%</li>
<li>“absolutely” 贡献了 10% → 所以你可以知道
<strong>哪个词对情绪判断最重要</strong>。</li>
</ul>
</blockquote>
<h4 id="limitations-1">Limitations</h4>
<p>While helpful, these methods can sometimes be <mark>computationally
expensive</mark> or provide explanations that are <mark>not
intuitive</mark> for end-users, depending on the complexity of the model
and the data involved.</p>
<h3 id="example-based-explanations">3. Example-based Explanations</h3>
<blockquote>
<p>Example-based explanations in XAI provide insights into model
behaviour by <mark>highlighting specific instances from the data
set</mark>.
展示模型的“原型样本”、“边界异常样本”或“反事实样本”来解释行为</p>
</blockquote>
<h4 id="method-1prototypes-and-criticisms">Method 1:<strong>Prototypes
and Criticisms</strong></h4>
<p>原型（Prototype）：最能代表某类别的典型样本</p>
<p><img src="./f8kj36.png"
alt="image-20250620090602037" />批评样本（Criticism)：属于该类但有些异常的“边界”样本.</p>
<h4 id="method-2-counterfactual-explanations">Method 2:
<strong>Counterfactual Explanations</strong></h4>
<p>反事实：<mark style="background-color: #90ee90;"> what if</mark> I
did…, can I get diff outcome?</p>
<p>-&gt; 如果我有100k，我会干嘛干嘛吗？</p>
<h4 id="limitations-2">Limitations</h4>
<p>需精心设计 what-if问题</p>
<h4 id="method-3-contrastive-explanations">Method 3: <strong>Contrastive
Explanations</strong></h4>
<p>强调模型为什么选择了结果 A，而不是 B</p>
<p><mark style="background-color: #90ee90;"> Why a, not b?</mark></p>
<h2 id="ethical-philosophical-considerations">Ethical &amp;
Philosophical Considerations</h2>
<h3 id="principles-for-ai-in-society">Principles for AI in Society</h3>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>原则名</th>
<th>讲义内容简述（slides）</th>
<th>讲者解释与扩展（transcript）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beneficence</td>
<td>Promote well-being, dignity, sustainability</td>
<td>&gt; “AI should promote well-being, preserve dignity and sustain the
planet.”→ AI 应该被用来做善事、促进人类福祉</td>
</tr>
<tr>
<td>Non-maleficence（no harm）</td>
<td>Privacy, Security, Capability Caution</td>
<td>&gt; “Don’t do bad… avoid harm such as violating privacy.”→
包括尊重隐私、安全、谨慎发展强 AI</td>
</tr>
<tr>
<td>Autonomy</td>
<td>The power to decide, informed consent</td>
<td>&gt; “Like informed consent in medicine… users should consent to how
AI is used for them.”</td>
</tr>
<tr>
<td>Justice</td>
<td>Prosperity, solidarity, fairness</td>
<td>&gt; “Promoting prosperity, avoiding unfairness… like algorithmic
bias”→ 我们在“Bias”那一讲重点讨论过</td>
</tr>
<tr>
<td>Explicability</td>
<td>Enabling the other principles through:</td>
<td></td>
</tr>
<tr>
<td>Intelligibilit<br />可理解性</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Accountability</td>
<td>“Explicability is foundational. It underpins all the other
principles.”</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="mind-map">Mind Map</h2>
<ul>
<li><p>Different stakeholders with different explainability
requirements</p></li>
<li><p>Goals:</p>
<p>‒ improvedecision-making</p>
<p>‒ ethics and accountability</p></li>
<li><p>Human and technical challenges:</p></li>
<li><p>opacity</p></li>
<li><p>causality</p></li>
<li><p>human interpretation, non-expert end users</p></li>
<li><p>Properties:</p></li>
<li><p>local versus global</p></li>
<li><p>interpretable versus post-hoc</p></li>
<li><p>model-agnostic versus model-specific</p></li>
<li><p>Methods:</p>
<ul>
<li><p>rule-based</p></li>
<li><p>attribution-based</p></li>
<li><p>example-based</p></li>
</ul></li>
</ul>
        </div>
      </div>
    </div>
    
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="https://maxcdn.bootstrapcdn.com/js/ie10-viewport-bug-workaround.js"></script>
    <script>
        //document.getElementById('sidebar').getElementsByTagName('ul')[0].className += "nav nav-sidebar";
        
        /* ajust the height when click the toc
           the code is from https://github.com/twbs/bootstrap/issues/1768
        */
        var shiftWindow = function() { scrollBy(0, -50) };
        window.addEventListener("hashchange", shiftWindow);
        function load() { if (window.location.hash) shiftWindow(); }
        
        /*add Bootstrap styles to tables*/
        var tables = document.getElementsByTagName("table");
        for(var i = 0; i < tables.length; ++i){
            tables[i].className += "table table-bordered table-hover";
        }

        /* Add quote styles */
  
        document.addEventListener("DOMContentLoaded", function () {
          document.querySelectorAll("blockquote").forEach(function (block) {
            const first = block.querySelector("p");
            if (!first) return;
            const text = first.textContent.trim();

            if (text.startsWith("[!NOTE]")) {
              block.classList.add("note");
              first.innerHTML = "<strong>NOTE</strong>" + first.innerHTML.replace("[!NOTE]", "");
            } else if (text.startsWith("[!TIP]")) {
              block.classList.add("tip");
              first.innerHTML = "<strong>TIP</strong>" + first.innerHTML.replace("[!TIP]", "");
            } else if (text.startsWith("[!WARNING]")) {
              block.classList.add("warning");
              first.innerHTML = "<strong>WARNING</strong>" + first.innerHTML.replace("[!WARNING]", "");
            }
          });
        });

        document.addEventListener('DOMContentLoaded', function () {
          var links = document.querySelectorAll('a[href]');
          for (var i = 0; i < links.length; i++) {
            var href = links[i].getAttribute('href');
            if (!href) continue;

            if (href.indexOf('.md#') !== -1) {
              var parts = href.split('.md#');
              var newHref = parts[0] + '.html#' + parts[1];
              links[i].setAttribute('href', newHref);
            } else if (href.length > 3 && href.slice(-3) === '.md') {
              var newHref = href.slice(0, -3) + '.html';
              links[i].setAttribute('href', newHref);
            }
          }
        });




    </script>
  </body>
</html>
